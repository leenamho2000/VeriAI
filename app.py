# app.py â€” Final version with all features including SHAP
from dotenv import load_dotenv
load_dotenv()

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import re as _re
from rapidfuzz import fuzz
from PIL import Image
from pathlib import Path
import platform
import matplotlib.patches

# --- SHAP/XAI ê¸°ëŠ¥ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
import shap
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

from rules import analyze_text, W
from llm import analyze_ad, analyze_report
from parsers import extract_text_from_url
from report import export_pdf

st.set_page_config(page_title="VeriAI â€” ë¬¸ì„œ ì‹ ë¢°ë„/ê·¼ê±° ë¶„ì„ AI", layout="wide")

# ====================== í•œê¸€ í°íŠ¸ ì„¤ì • (ìµœì¢… ìˆ˜ì • ë²„ì „) ======================
def _setup_korean_font():
    """ì‹œìŠ¤í…œì— ë§ëŠ” í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì•„ matplotlibì— ì„¤ì •í•©ë‹ˆë‹¤. ì•±ì´ ì¤‘ë‹¨ë˜ì§€ ì•Šë„ë¡ ì˜ˆì™¸ ì²˜ë¦¬ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."""
    try:
        font_path_to_use = None
        
        # 1. í”„ë¡œì íŠ¸ ë‚´ fonts í´ë”ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
        local_font_candidates = [
            Path("./fonts/NotoSansKR-Regular.otf"),
            Path("./fonts/NanumGothic.ttf")
        ]
        for font_path in local_font_candidates:
            if font_path.exists():
                font_path_to_use = str(font_path)
                break

        # 2. ë¡œì»¬ í°íŠ¸ê°€ ì—†ìœ¼ë©´ ì‹œìŠ¤í…œ í°íŠ¸ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
        if not font_path_to_use:
            system_name = platform.system()
            if system_name == "Windows":
                system_font_path = Path("c:/Windows/Fonts/malgun.ttf")
            elif system_name == "Darwin":  # macOS
                system_font_path = Path("/System/Library/Fonts/AppleSDGothicNeo.ttc")
            elif system_name == "Linux":
                system_font_path = Path("/usr/share/fonts/truetype/nanum/NanumGothic.ttf")
            else:
                system_font_path = None
            
            if system_font_path and system_font_path.exists():
                font_path_to_use = str(system_font_path)

        # 3. ì°¾ì€ í°íŠ¸ë¥¼ Matplotlibì— ì„¤ì •í•©ë‹ˆë‹¤. (ê°€ì¥ ì•ˆì •ì ì¸ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •)
        if font_path_to_use:
            fm.fontManager.addfont(font_path_to_use)
            font_name = fm.FontProperties(fname=font_path_to_use).get_name()
            
            # rcParamsë¥¼ í•œ ë²ˆì— ì—…ë°ì´íŠ¸í•˜ì—¬ ì„¤ì • ì¶©ëŒ ê°€ëŠ¥ì„±ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.
            plt.rcParams.update({
                "font.family": font_name,
                "axes.unicode_minus": False,
            })
        else:
            print("Warning: Korean font not found. SHAP plot may display Korean characters as squares.")

    except Exception as e:
        # í°íŠ¸ ì„¤ì • ì¤‘ ì–´ë–¤ ì—ëŸ¬ê°€ ë°œìƒí•˜ë”ë¼ë„ ì•±ì´ ì£½ì§€ ì•Šë„ë¡ ë°©ì§€í•©ë‹ˆë‹¤.
        print(f"Error setting up Korean font: {e}")
        print("Warning: Proceeding without custom font settings due to an error.")
        
_setup_korean_font()

# ====================== STYLE ======================
CUSTOM_CSS = """
<style>
.badge{display:inline-block;padding:4px 8px;border-radius:999px;margin:2px 6px 2px 0;font-size:12px;background:#eef1f5;}
.badge.red{background:#ffe4e4;} .badge.orange{background:#fff0e0;} .badge.green{background:#eaf7ea;}
mark{background:#fff3a6;padding:0 2px;} .small-muted{color:#6b7280;font-size:12px;}
</style>
"""
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)

# ====================== STATE ======================
def _init_state():
    defaults = { "ruleset": "ad", "text_input": "", "url_input": "", "url_error": "", "df": None, "k": 5, "min_risk": 40, "allowed_labels": ("High", "Medium"), "similarity_threshold": 85, "llm_results": None, }
    for k, v in defaults.items():
        if k not in st.session_state: st.session_state[k] = v
_init_state(); st.session_state._re_sub = _re.sub

# ====================== HELPERS ======================
AXES = ["evidence_inverse", "vagueness", "language", "coverage", "temporal", "offset_risk"]
AXES_KO = ["ê·¼ê±°ì„±(ì—­)", "ëª¨í˜¸ì„±", "ì–¸ì–´ì  ìœ„í—˜", "ì ìš©ë²”ìœ„ ìœ„í—˜", "ì‹œì /ê¸°ê°„ ìœ„í—˜", "ì˜¤í”„ì…‹ ì˜ì¡´ë„"]
def _as_dict(x): return x if isinstance(x, dict) else {}
def _highlight_sentence(text: str, hits_like) -> str:
    hits = _as_dict(hits_like)
    all_hits = []
    for vals in hits.values():
        if isinstance(vals, list):
            all_hits.extend([v for v in vals if v])

    if not all_hits:
        return str(text)

    uniq = sorted(set(all_hits), key=lambda x: (-len(x), x))
    pat = "|".join(map(_re.escape, uniq))

    if not pat:
        return str(text)
        
    return st.session_state._re_sub(pat, lambda m: f"<mark>{m.group(0)}</mark>", str(text))
def _component_values(row):
    ev_inv=1-(row.get("evidence_score",0)/16);vag=(row.get("vagueness_score",0)/16);lang=(row.get("language_risk",0)/8);cov=(row.get("coverage_penalty",0)/6);tmp=(row.get("temporal_penalty",0)/4);off=float(row.get("offset_flag",0))
    return np.clip(np.array([ev_inv, vag, lang, cov, tmp, off]), 0, 1)
def _weighted_contrib(row):
    base=_component_values(row);weights=np.array([W.get(k,0) for k in AXES]);parts=100*base*weights;total=parts.sum();risk=float(row.get("risk") or 0)
    if total > 0 and risk > 0: parts=parts*(risk/total)
    return parts

# SHAPì„ ìœ„í•œ Helper í•¨ìˆ˜: ì´ë¯¸ ì¶”ì¶œëœ íŠ¹ì§•(dict)ìœ¼ë¡œ ì ìˆ˜ë¥¼ ê³„ì‚°
def score_sentence_from_features(features: dict) -> float:
    f = features
    risk = 100 * (
        W.get("evidence_inverse", 0.30) * (1 - f.get("evidence_score", 0) / 16)
        + W.get("vagueness", 0.22) * (f.get("vagueness_score", 0) / 16)
        + W.get("language", 0.12) * (f.get("language_risk", 0) / 8)
        + W.get("coverage", 0.10) * (f.get("coverage_penalty", 0) / 6)
        + W.get("temporal", 0.16) * (f.get("temporal_penalty", 0) / 4)
        + W.get("offset_risk", 0.10) * f.get("offset_flag", 0)
    )
    return max(0, min(100, risk))

# ====================== UI LAYOUT ======================
def on_click_fetch_url():
    url = (st.session_state.get("url_input") or "").strip()
    if not url: st.session_state["url_error"] = "URLì„ ì…ë ¥í•˜ì„¸ìš”."; return
    try:
        with st.spinner("URLì—ì„œ ë³¸ë¬¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘â€¦"): fetched = extract_text_from_url(url, max_paragraphs=16)
        fetched = fetched.replace("\uFFFD", " "); st.session_state["text_input"] = fetched; st.session_state["url_error"] = ""
    except Exception as e: st.session_state["url_error"] = f"URL ì½ê¸° ì‹¤íŒ¨: {e}"

st.title("ğŸ§  VeriAI â€” ë¬¸ì„œ ì‹ ë¢°Â·ê·¼ê±° ìë™ ë¶„ì„"); st.caption("ë¬¸ì¥ ë‹¨ìœ„ ê·œì¹™ ì ìˆ˜í™” â†’ ìƒìœ„ ìœ„í—˜ë¬¸ì¥ë§Œ LLMìœ¼ë¡œ ê·¼ê±°/ë³´ì™„ ì œì•ˆ (ESG/ê´‘ê³ /ì¼ë°˜ ë³´ê³ ì„œ ì „ë¶€ ì§€ì›)")

with st.sidebar:
    st.header("ì„¤ì •")
    mode = st.radio(
        "ë¶„ì„ ëª¨ë“œ",
        ["í™˜ê²½ ê´‘ê³  (Ad)", "ì¼ë°˜ ë³´ê³ ì„œ (Report)"],
        index=0 if st.session_state.ruleset == "ad" else 1,
        help="ë¶„ì„í•  ë¬¸ì„œì˜ ì¢…ë¥˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. 'í™˜ê²½ ê´‘ê³ 'ëŠ” ê·¸ë¦°ì›Œì‹± íƒì§€ì—, 'ì¼ë°˜ ë³´ê³ ì„œ'ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤/ê¸°ìˆ  ë³´ê³ ì„œì˜ ê·¼ê±°ì„± ì ê²€ì— ìµœì í™”ëœ ê·œì¹™ì„ ì ìš©í•©ë‹ˆë‹¤."
    )
    st.session_state.ruleset = "ad" if mode.startswith("í™˜ê²½") else "report"
    st.session_state.k = st.slider(
        "Top-K (LLM ëŒ€ìƒ)", 1, 10, st.session_state.k,
        help="ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ëœ ë¬¸ì¥ ì¤‘, ìœ„í—˜ë„ê°€ ê°€ì¥ ë†’ì€ Kê°œì˜ ë¬¸ì¥ì„ ì„ ì •í•˜ì—¬ LLM(AI)ì—ê²Œ ì‹¬ì¸µ ë¶„ì„ì„ ìš”ì²­í•©ë‹ˆë‹¤. LLMì€ ì™œ ìœ„í—˜í•œì§€, ì–´ë–¤ ê·¼ê±°ê°€ ë³´ê°•ë˜ì–´ì•¼ í•˜ëŠ”ì§€ ë“±ì„ ì œì•ˆí•©ë‹ˆë‹¤."
    )
    st.session_state.min_risk = st.slider(
        "ìµœì†Œ ìœ„í—˜ë„", 0, 100, st.session_state.min_risk, step=5,
        help="LLM ì‹¬ì¸µ ë¶„ì„ ëŒ€ìƒìœ¼ë¡œ ê³ ë ¤í•  ë¬¸ì¥ì˜ ìµœì†Œ ìœ„í—˜ë„ ì ìˆ˜(0-100)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì´ ì ìˆ˜ ë¯¸ë§Œì¸ ë¬¸ì¥ì€ LLM ë¶„ì„ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤."
    )
    st.session_state.allowed_labels = tuple(st.multiselect(
        "í¬í•¨ ë¼ë²¨", ["High", "Medium", "Low"],
        default=list(st.session_state.allowed_labels),
        help="LLM ì‹¬ì¸µ ë¶„ì„ ëŒ€ìƒìœ¼ë¡œ ê³ ë ¤í•  ìœ„í—˜ë„ ë¼ë²¨(High, Medium, Low)ì„ ì„ íƒí•©ë‹ˆë‹¤. ì„ íƒëœ ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì¥ë§Œ ë¶„ì„ ëŒ€ìƒì´ ë©ë‹ˆë‹¤."
    ))
    st.session_state.similarity_threshold = st.slider(
        "ìœ ì‚¬ë¬¸ì¥ ì œê±° ë¯¼ê°ë„", 70, 100, st.session_state.similarity_threshold, step=1,
        help="LLM ë¶„ì„ ëŒ€ìƒ ì„ ì • ì‹œ, ë‚´ìš©ì´ ìœ ì‚¬í•œ ë¬¸ì¥ë“¤ì´ ì¤‘ë³µìœ¼ë¡œ ë½‘íˆì§€ ì•Šë„ë¡ ì œê±°í•©ë‹ˆë‹¤. ë¯¼ê°ë„ê°€ ë†’ì„ìˆ˜ë¡ ì•½ê°„ì˜ ì°¨ì´ë§Œ ìˆì–´ë„ ë‹¤ë¥¸ ë¬¸ì¥ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤."
    )
    st.markdown("---"); st.markdown("**LLM ì‚¬ìš© ì•ˆë‚´**\n- `OPENAI_API_KEY` í•„ìš”\n- ê´‘ê³ : â€˜ì™œ ìœ„í—˜ì¸ì§€ + ê²€ì¦ ì¿¼ë¦¬â€™\n- ë³´ê³ ì„œ: â€˜ë¬´ì—‡ì„ ì¶”ê°€í• ì§€(ì§€í‘œ/ë°©ë²•/ì¸ìš©)â€™")

st.subheader("1) í…ìŠ¤íŠ¸/URL ì…ë ¥"); col1, col2 = st.columns([2,1])
with col1: st.text_area("ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë™ ë¶„í• /ì •ê·œí™”í•©ë‹ˆë‹¤.", key="text_input", height=220, placeholder="ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.")
with col2:
    st.text_input("ë˜ëŠ” URL ì…ë ¥", key="url_input", placeholder="https://example.com/article")
    st.button("ğŸŒ URL ë³¸ë¬¸ ë¶ˆëŸ¬ì˜¤ê¸°", use_container_width=True, on_click=on_click_fetch_url)
    if st.session_state.get("url_error"): st.error(st.session_state["url_error"])
run = st.button("ğŸ” ë¶„ì„í•˜ê¸°", type="primary")

@st.cache_data(show_spinner=False)
def _analyze(text: str, ruleset: str): return pd.DataFrame(analyze_text(text, ruleset=ruleset))

if run:
    txt = (st.session_state.text_input or "").strip()
    if not txt: st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ URLì„ ë¶ˆëŸ¬ì˜¤ì„¸ìš”.")
    else:
        df_raw = _analyze(txt, st.session_state.ruleset); df_raw.insert(0, 'ë²ˆí˜¸', range(1, len(df_raw) + 1))
        st.session_state.df = df_raw; st.session_state.llm_results = None

df = st.session_state.df
# ====================== OUTPUT ======================
if isinstance(df, pd.DataFrame) and not df.empty:
    avg_risk = round(float(df["risk"].mean()), 1); high_cnt = int((df.get("label") == "High").sum())
    c1,c2,c3,c4 = st.columns(4); c1.metric("í‰ê·  ìœ„í—˜ë„",f"{avg_risk}"); c2.metric("High ë¬¸ì¥ ìˆ˜",f"{high_cnt}"); c3.metric("ì´ ë¬¸ì¥ ìˆ˜",f"{len(df)}"); c4.metric("ë¶„ì„ ëª¨ë“œ", "í™˜ê²½ ê´‘ê³ " if st.session_state.ruleset == "ad" else "ì¼ë°˜ ë³´ê³ ì„œ")
    st.subheader("2) ê²°ê³¼ íƒìƒ‰"); tab1, tab2, tab3, tab4 = st.tabs(["ê°œìš”(í‘œ)", "ë¬¸ì¥ë³„ íƒìƒ‰", "ì‹œê°í™”", "ë‚´ë³´ë‚´ê¸°"])

    with tab1:
        show = df.copy().sort_values("risk", ascending=False); q = st.text_input("ë¬¸ì¥ ê²€ìƒ‰(í‚¤ì›Œë“œ)", "")
        if q: show = show[show["sentence"].astype(str).str.contains(q, case=False, na=False)]
        
        rename_map = {
            "sentence": "ë¬¸ì¥", "risk": "ìœ„í—˜ë„", "label": "ë“±ê¸‰",
            "evidence_score": "ê·¼ê±° ì ìˆ˜", "vagueness_score": "ëª¨í˜¸ì„± ì ìˆ˜"
        }
        show.rename(columns=rename_map, inplace=True)
        
        cols = ["ë²ˆí˜¸", "ë¬¸ì¥", "ìœ„í—˜ë„", "ë“±ê¸‰", "ê·¼ê±° ì ìˆ˜", "ëª¨í˜¸ì„± ì ìˆ˜"]
        cols = [c for c in cols if c in show.columns]
        
        cfg = {"ìœ„í—˜ë„": st.column_config.ProgressColumn("ìœ„í—˜ë„", min_value=0, max_value=100, format="%.1f")}
        
        st.dataframe(show[cols], use_container_width=True, column_config=cfg)
        st.caption("â€» ê·¼ê±° ì ìˆ˜: ë¬¸ì¥ì— ìˆ˜ì¹˜, ì—°ë„, ì¶œì²˜, ì™¸ë¶€ ê²€ì¦ ë“± êµ¬ì²´ì ì¸ ê·¼ê±°ê°€ ë§ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤.")

    with tab2:
        options_map = {row['ë²ˆí˜¸']: f"{row['ë²ˆí˜¸']}. {str(row['sentence'])[:70]}..." for _, row in df.iterrows()}
        selected_num = st.selectbox("ë¬¸ì¥ ì„ íƒ", options=df['ë²ˆí˜¸'].tolist(), format_func=lambda num: options_map[num])
        row = df[df['ë²ˆí˜¸'] == selected_num].iloc[0].to_dict()

        st.markdown("**ì›ë¬¸**"); st.markdown(_highlight_sentence(row.get("sentence",""), row.get("hits")), unsafe_allow_html=True)
        a, b, c = st.columns(3); a.metric("ìœ„í—˜ë„", f"{row.get('risk',0):.1f}"); b.metric("ë“±ê¸‰", str(row.get('label',''))); c.metric("ê·¼ê±° ì ìˆ˜", f"{int(row.get('evidence_score',0))}/16")
        with st.expander("ğŸ” ê·œì¹™ ë§¤ì¹­ ìƒì„¸ (íˆíŠ¸ ë‹¨ì–´ ë³´ê¸°)"):
            hits = _as_dict(row.get("hits"))
            def chips(items, tone=""):
                if not items: return
                tone_cls = {"red":"red", "orange":"orange", "green":"green"}.get(tone, "")
                st.markdown(" ".join([f"<span class='badge {tone_cls}'>{st.session_state._re_sub(r'\\s+', '&nbsp;', str(it))}</span>" for it in items]), unsafe_allow_html=True)
            st.write("**ëª¨í˜¸ì–´**"); chips(hits.get("vague", []), "orange"); st.write("**ê³¼ì¥í‘œí˜„**"); chips(hits.get("overclaim", []), "red"); st.write("**ë¯¸ë˜ì‹œì œ/ê³„íš**"); chips(hits.get("future", [])); st.write("**ë²”ìœ„-ìœ„í—˜**"); chips(hits.get("coverage_risky", []), "red"); st.write("**ë²”ìœ„-ì™„í™”(ëª…í™•í™”)**"); chips(hits.get("coverage_clarifier", []), "green"); st.write("**í‘œì¤€/ë°©ë²•**"); chips(hits.get("standards_method", []), "green"); st.write("**ì œ3ì/ê²€ì¦**"); chips(hits.get("third_party", []), "green"); st.write("**ì˜¤í”„ì…‹/í¬ë ˆë”§**"); chips(hits.get("offset_terms", []), "orange")

        st.markdown("#### AI íŒë‹¨ ê·¼ê±° ë¶„ì„ (SHAP Waterfall Plot)")
        with st.spinner("SHAP ë¶„ì„ì„ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤..."):
            shap_features = [
                'evidence_score', 'vagueness_score', 'coverage_penalty', 
                'temporal_penalty', 'language_risk', 'offset_flag'
            ]
            feature_name_map_ko = {
                'evidence_score': 'ê·¼ê±° ì ìˆ˜', 'vagueness_score': 'ëª¨í˜¸ì„± ì ìˆ˜',
                'coverage_penalty': 'ì ìš©ë²”ìœ„ ìœ„í—˜', 'temporal_penalty': 'ì‹œì /ê¸°ê°„ ìœ„í—˜',
                'language_risk': 'ì–¸ì–´ì  ìœ„í—˜', 'offset_flag': 'ì˜¤í”„ì…‹ ì˜ì¡´'
            }
            shap_features_ko = [feature_name_map_ko[f] for f in shap_features]

            instance_values = pd.Series(row)[shap_features].values.astype(float)
            background_data = df[shap_features].sample(min(50, len(df)))

            explainer = shap.KernelExplainer(
                lambda x: pd.DataFrame(x, columns=shap_features).apply(
                    lambda s: score_sentence_from_features(dict(s)), axis=1
                ).values, 
                background_data
            )
            shap_values = explainer.shap_values(instance_values)
            
            fig, ax = plt.subplots(figsize=(12, 5), dpi=150)
            
            shap_exp = shap.Explanation(
                values=shap_values, base_values=explainer.expected_value,
                data=instance_values, feature_names=shap_features_ko
            )
            
            shap.plots.waterfall(shap_exp, max_display=10, show=False)

            # --- ê·¸ë˜í”„ ìƒ‰ìƒ ë° ë¶€í˜¸ Failsafe ë¡œì§ ---
            # SHAP ê°’ì— ë”°ë¼ ë§‰ëŒ€ ìƒ‰ìƒì„ ê°•ì œë¡œ ì¬ì„¤ì •
            shap_values_in_plot_order = shap_exp.values[np.argsort(np.abs(shap_exp.values))][-10:]
            bar_artists = [p for p in ax.patches if isinstance(p, matplotlib.patches.Rectangle) and p.get_height() < 1.0]
            bar_artists.sort(key=lambda p: p.get_y())
            
            if len(bar_artists) == len(shap_values_in_plot_order):
                for artist, value in zip(bar_artists, shap_values_in_plot_order):
                    artist.set_facecolor(shap.plots.colors.red_rgb if value > 0 else shap.plots.colors.blue_rgb)

            # ë§‰ëŒ€ ìœ„ì˜ ìˆ«ì ë ˆì´ë¸”ì—ì„œ ë¶€í˜¸(+/-)ë¥¼ ëª¨ë‘ ì œê±°
            for text_obj in ax.texts:
                label = text_obj.get_text()
                cleaned_label = label.lstrip('+âˆ’-')
                if label != cleaned_label:
                    try:
                        float(cleaned_label)
                        text_obj.set_text(cleaned_label)
                    except ValueError:
                        pass

            xmin, xmax = ax.get_xlim()
            padding = (xmax - xmin) * 0.1
            ax.set_xlim(xmin - padding, xmax + padding)
            plt.tight_layout(pad=1.5)
            st.pyplot(fig, clear_figure=True)

        with st.expander("ğŸ’¡ ì°¨íŠ¸ í•´ì„ ë°©ë²• (ê³ ì • ì•ˆë‚´ë¬¸)"):
            st.markdown("""
            ì´ ì°¨íŠ¸ëŠ” **AIê°€ ê³„ì‚°í•œ ìœ„í—˜ë„ ì ìˆ˜ê°€ ì–´ë–»ê²Œ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€** ê° ìš”ì¸ë³„ë¡œ ìƒì„¸íˆ ë³´ì—¬ì¤ë‹ˆë‹¤.

            - **`E[f(X)]` (íšŒìƒ‰ ê¸°ì¤€ì„ )**: ì´ ë¬¸ì„œì— ìˆëŠ” ë¬¸ì¥ë“¤ì˜ í‰ê· ì ì¸ ìœ„í—˜ë„ ì ìˆ˜ì…ë‹ˆë‹¤. ëª¨ë“  ë¶„ì„ì€ ì´ í‰ê·  ì ìˆ˜ì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤.
            - **`f(x)` (ìµœì¢… ì˜ˆì¸¡ ì ìˆ˜)**: í˜„ì¬ ì„ íƒëœ ë¬¸ì¥ì˜ ìµœì¢… ìœ„í—˜ë„ ì ìˆ˜ì…ë‹ˆë‹¤.
            - <span style='color:red;'>**ë¹¨ê°„ìƒ‰ ë§‰ëŒ€ (ì ìˆ˜ ìƒìŠ¹ ìš”ì¸) â†‘**</span>: ìœ„í—˜ë„ ì ìˆ˜ë¥¼ **ë†’ì´ëŠ”** ìš”ì¸ë“¤ì…ë‹ˆë‹¤.
            - <span style='color:blue;'>**íŒŒë€ìƒ‰ ë§‰ëŒ€ (ì ìˆ˜ í•˜ë½ ìš”ì¸) â†“**</span>: ìœ„í—˜ë„ ì ìˆ˜ë¥¼ **ë‚®ì¶”ëŠ”** ìš”ì¸ë“¤ì…ë‹ˆë‹¤.
            - **ë§‰ëŒ€ì˜ ê¸¸ì´**: ê° ìš”ì¸ì´ ì ìˆ˜ì— ë¯¸ì¹œ ì˜í–¥ë ¥ì˜ í¬ê¸°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
            """, unsafe_allow_html=True)
            
    with tab3:
        parts_avg=np.mean([_weighted_contrib(r) for _,r in df.iterrows()],axis=0); contrib_sorted=sorted(zip(AXES_KO,parts_avg),key=lambda x:-x[1]); top_two_risks=[item[0] for item in contrib_sorted[:2]]; st.info(f"**ë¬¸ì„œ ì „ì²´ì˜ ì£¼ìš” ìœ„í—˜ ìš”ì¸:** {top_two_risks[0]}, {top_two_risks[1]}")
        st.markdown("#### ë¬¸ì¥ë³„ ìœ„í—˜ë„ ë¶„í¬ (Scatter Plot)"); scatter_df=df.copy(); scatter_df['ìš”ì•½']=scatter_df['sentence'].str.slice(0,80)+'...'; color_map={'High':'red','Medium':'orange','Low':'skyblue'}; fig_scatter=px.scatter(scatter_df,x='ë²ˆí˜¸',y='risk',color='label',color_discrete_map=color_map,hover_data=['ìš”ì•½'],title='ë¬¸ì¥ ìœ„ì¹˜ë³„ ìœ„í—˜ë„ ì ìˆ˜',labels={'ë²ˆí˜¸':'ë¬¸ì¥ ë²ˆí˜¸','risk':'ìœ„í—˜ë„ ì ìˆ˜'}); st.plotly_chart(fig_scatter,use_container_width=True)
        st.markdown("#### ë¬¸ì¥ë³„ ìœ„í—˜ ìš”ì†Œ ê¸°ì—¬ë„ (Stacked Bar Chart)"); contrib_data=pd.DataFrame([_weighted_contrib(row) for _,row in df.iterrows()],columns=AXES_KO); contrib_data['ë²ˆí˜¸']=contrib_data.index+1; contrib_df_melted=contrib_data.melt(id_vars='ë²ˆí˜¸',var_name='ìœ„í—˜ ìš”ì†Œ',value_name='ê¸°ì—¬ë„'); fig_stacked_bar=px.bar(contrib_df_melted,x='ë²ˆí˜¸',y='ê¸°ì—¬ë„',color='ìœ„í—˜ ìš”ì†Œ',title='ê° ë¬¸ì¥ì˜ ìœ„í—˜ë„ ì ìˆ˜ êµ¬ì„± ìš”ì†Œ',labels={'ë²ˆí˜¸':'ë¬¸ì¥ ë²ˆí˜¸','ê¸°ì—¬ë„':'ìœ„í—˜ë„ ê¸°ì—¬ë„'}); st.plotly_chart(fig_stacked_bar,use_container_width=True)

    with tab4:
        work = df.copy()
        if "label" in work.columns:
            work = work[work["label"].isin(st.session_state.allowed_labels)]
        work = work[work["risk"] >= st.session_state.min_risk].sort_values("risk", ascending=False).reset_index(drop=True)

        if work.empty:
            st.warning("ì„¤ì • ê¸°ì¤€ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            selected = []
            for _, r in work.iterrows():
                s = r["sentence"]
                if all(fuzz.ratio(s, prev) < st.session_state.similarity_threshold for prev in selected):
                    selected.append(s)
                    if len(selected) >= st.session_state.k:
                        break
            
            topk = work[work["sentence"].isin(selected)].copy().sort_values("risk", ascending=False)
            
            view_cols = ["ë²ˆí˜¸", "sentence", "risk", "label"]
            view = df[df['sentence'].isin(topk['sentence'].tolist())][view_cols].sort_values('risk', ascending=False)

            view_display = view.copy()
            view_display.rename(columns={"sentence": "ë¬¸ì¥", "risk": "ìœ„í—˜ë„", "label": "ë“±ê¸‰"}, inplace=True)
            st.dataframe(view_display, use_container_width=True)
            st.info("ìœ„ ëª©ë¡ë§Œ LLM í›„ì²˜ë¦¬ ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

            colx, coly = st.columns(2)
            items_list = [{"id": int(r.ë²ˆí˜¸), "text": r.sentence, "risk": float(r.risk), "label": r.label} for r in view.itertuples(index=False)]
            hashable_items = tuple(tuple(sorted(d.items())) for d in items_list)

            if st.session_state.ruleset == "ad":
                if colx.button("ğŸ” LLM ê·¼ê±°Â·ìœ„í—˜ ë¶„ì„ ì‹¤í–‰ (ê´‘ê³ )", use_container_width=True):
                    try:
                        st.session_state.llm_results = analyze_ad(hashable_items)
                        st.success("LLM ë¶„ì„ ì™„ë£Œ")
                    except Exception as e:
                        st.error(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            else:
                if colx.button("ğŸ§© LLM ì¦ë¹™ ë³´ì™„ ì œì•ˆ ì‹¤í–‰ (ë³´ê³ ì„œ)", use_container_width=True):
                    try:
                        st.session_state.llm_results = analyze_report(hashable_items)
                        st.success("LLM ë¶„ì„ ì™„ë£Œ")
                    except Exception as e:
                        st.error(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")

            if isinstance(st.session_state.llm_results, list) and st.session_state.llm_results:
                st.markdown("#### LLM ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
                id2sent = {int(r["id"]): r["text"] for r in items_list}
                disp_data = []
                for res in st.session_state.llm_results:
                    res_id = res.get("id")
                    disp_item = {"ë²ˆí˜¸": res_id, "ë¬¸ì¥": id2sent.get(res_id, "")}
                    if st.session_state.ruleset == "ad":
                        disp_item["ìœ„í—˜ ì‚¬ìœ "] = ", ".join(res.get("risk_reasons", []))
                        disp_item["ìƒì„¸ ì„¤ëª…"] = res.get("explanation", "")
                    else:
                        disp_item["ì£¼ìš” ì´ìŠˆ"] = ", ".join(res.get("issues", []))
                    disp_data.append(disp_item)
                st.dataframe(pd.DataFrame(disp_data), use_container_width=True)

            csv = df.to_csv(index=False).encode("utf-8-sig")
            st.download_button("â¬‡ï¸ ì „ì²´ ê²°ê³¼ CSV", csv, "veriai_results.csv", "text/csv", use_container_width=True)

            if coly.button("ğŸ–¨ï¸ PDF ë¦¬í¬íŠ¸ ìƒì„±", use_container_width=True):
                with st.spinner("PDF ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    try:
                        summary = {
                            "ë¶„ì„ ëª¨ë“œ": "í™˜ê²½ ê´‘ê³ " if st.session_state.ruleset=="ad" else "ì¼ë°˜ ë³´ê³ ì„œ",
                            "í‰ê·  ìœ„í—˜ë„": avg_risk,
                            "'High' ë“±ê¸‰ ë¬¸ì¥ ìˆ˜": high_cnt,
                            "ì´ ë¬¸ì¥ ìˆ˜": len(df)
                        }
                        outputs = []
                        if isinstance(st.session_state.llm_results, list):
                            id2sent = {int(r.ë²ˆí˜¸): r.sentence for r in view.itertuples(index=False)}
                            for obj in st.session_state.llm_results:
                                outputs.append({ "sentence": id2sent.get(int(obj.get("id")), ""), "result": obj })
                        
                        path = export_pdf(summary, df.to_dict("records"), outputs, path="veriai_report.pdf", visuals=None)
                        
                        st.success("PDF ìƒì„± ì™„ë£Œ!")
                        with open(path, "rb") as f:
                            st.download_button("â¬‡ï¸ PDF ë‹¤ìš´ë¡œë“œ", f, file_name="veriai_report.pdf", mime="application/pdf")
                    except Exception as e:
                        st.error(f"PDF ìƒì„± ì‹¤íŒ¨: {e}", icon="ğŸš¨")

